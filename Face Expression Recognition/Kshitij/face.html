<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Facial Expression Recognition(OpenCV)</title>
    <meta name="theme-color" content="rgb(255,255,255)">
    <meta name="description" content="Facial Expression Recognition is an application of Computer Vision, a boon of AI, which enables the computer recognise a person's emotion on the basis of his/her facial expression. Its main applcations are listed below:">
    <meta property="og:image" content="">
    <meta property="og:type" content="article">
    <meta name="keywords" content="SURF,Robust feature,robust,speeded-up">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.4.1/css/bootstrap.min.css">
    <link rel="manifest" href="manifest.json">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bitter">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Crete+Round">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="assets/fonts/fontawesome5-overrides.min.css">
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="stylesheet" href="assets/css/styles.css">
    <link rel="stylesheet" href="assets/css/regression.css">
    <link rel="stylesheet" href="assets/css/author.css">
</head>

<body>
    <nav class="navbar navbar-light navbar-expand-md sticky-top" id="navigation" style="padding: 25px 0px;background-color: white;">
        <div class="container-fluid container"><a class="navbar-brand" href="https://devincept.codes/" style="background-image: url(&quot;assets/img/CODES.gif&quot;);background-position: center;background-size: cover;background-repeat: no-repeat;width: 170px;height: 65px;"></a><button data-toggle="collapse" class="navbar-toggler"
                data-target="#navcol-1"><span class="sr-only">Toggle navigation</span><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse text-uppercase" id="navcol-1" style="font-family: Roboto, sans-serif;font-weight: bold;">
                <ul class="nav navbar-nav ml-auto" style="font-size: 13px;">
                    <li class="nav-item" role="presentation"><a class="nav-link active" href="https://devincept.codes/contribute.html">Contribute</a></li>
                    <li class="nav-item" role="presentation"><a class="nav-link active" href="https://devincept.tech/pricing.html">Free courses</a></li>
                    <li class="nav-item" role="presentation"><a class="nav-link" href="https://devincept.codes/sponsor.html">Sponsor us</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <section id="hero" style="background-color: #2f1b1b;color: rgb(255,255,255);">
        <div style="padding: 30px;">
            <p class="d-flex justify-content-center" style="font-family: Belgrano, serif;">&nbsp;OpenCV</p>
            <h1 class="d-flex justify-content-center topic-title" style="font-family: Belgrano, serif;">Facial Expression Recognition&nbsp;</h1>
        </div>
        <div id="base" style="background-color: rgba(255,255,255,0.1);padding: 10px 0;font-family: Belgrano, serif;">
            <p class="justify-content-center"><i class="fa fa-align-center"></i>&nbsp; Reads&nbsp;<span id="visits">0</span></p>
        </div>
    </section>
    <section id="sidetab" style="margin-bottom: 0px;">
        <div class="container-fluid">
            <div class="row content-row">
                <div class="col-3 author">
                    <div class="author-details"><div>
    <div class="author">
        
        <h2 class="author-title"> Author's Details </h2>
        
        <h3 class="author's-name"> Kshitij  Shrivastava </h3>
      
        
        <ul style="padding:0px;">
            
            <li><a href="https://www.linkedin.com/in/kshitij-shrivastava-a5aaa4118/"><i class="fab fa-linkedin"></i></a></li>
            <li><a href="https://twitter.com/KshitijShriva12"><i class="fab fa-twitter"></i></a></li>
             <li><a href="mailto:kshitijshrivastava36@gmail.com"><i class="fa fa-envelope"></i></a></li>
             <li><a href="https://github.com/kshitijshrivastava1903"><i class="fab fa-github"></i></a></li>
        </ul>
    </div>
</div></div>
                </div>
                <div class="col-9 main-content">
                    <div class="content-inner"><div>
  <div class="regression-content">
      <h1>Facial Expression Recognition using OpenCV and Keras</h1>
      <p>Facial Expression Recognition is an application of Computer Vision, a boon of AI, which enables the computer recognise a person's emotion on the basis of his/her facial expression. Its main applcations are listed below:</p>
      <h2>Applications:</h2>
      <ul>
<li>It can be used to detect the predominant emotion of a person while doing a particular activity like giving a lecture, watching a video, reading a post, etc.</li>
<li>It can be used to find how funny or emotional a particular video, or post, or image is by determing the facial expression of many people.</li>
<li>It can be used to detect mental disorders.</li>
<li>It can help prevent riots by detecting the anger emotion among multiple people.</li>
<li>It help the human users monitor their stress level.</li>
<li>It can help AI assistants to recommend videos or songs or say things according to the emotion of the user, like cheering them when they are sad and sounding happy,when a person is happy. This way, we can create good AI friends and help remove loneliness from people's lives.</li>
</ul>
      <h2>Aim of this project:</h2>
      <p>Training a convolutional neural network to recognise 7 kinds of emotions (Happy, Anger, Sad, Fear, Disgust, Surprise, Neutral) after detection of face in real time video camera and giving the prediction.</p>
      <h2>The dataset used for training and testing the model:</h2>
      <p>It consists of the training (28,709 images) and the testing (7,178 images) set comprising of images of people's faces with the 7 kinds of expressions mentioned above.
The class-distribution of the training set is as follows:</p>
      <ul>
  <li>3171 surprise images</li>
  <li>7215 happy images</li>
  <li>4965 neutral images</li>
  <li>3995 angry images</li>
  <li>4830 sad images</li>
  <li>436 disgust images</li>
  <li>4097 fear images</li>
</ul><br>
      <img src="https://camo.githubusercontent.com/b648e9fd03c9c2f3e9adf193bb847fcfb1a4f21c4e97172b48288a6b11b7fbac/68747470733a2f2f616c676f726974686d69612e636f6d2f626c6f672f77702d636f6e74656e742f75706c6f6164732f323031382f30322f66707379672d30362d30303736312d673030312e6a7067" data-canonical-src="https://algorithmia.com/blog/wp-content/uploads/2018/02/fpsyg-06-00761-g001.jpg" style="max-width:100%;height:auto;">
      <h2>A brief dicussion of Convolutional Neural Networks:</h2>
      <p>In neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories for image recognition, image classifications, object detection, face recognition, etc. These are some of the areas where CNNs are widely used. CNN's work on the principle of extracting low-level features in the earlier layers, like edges in the image, to more complex features in the deeper layers using complex mathematical operations (called convolving) on the pixel values of the given image using mathematical entities called filters.</p>
      <img src="https://camo.githubusercontent.com/bdcd1880f6a345faf7a7d6c732d536c09db9d20049628ece3a9cb003187b2943/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f70726f78792f312a586275573857755272415935704334742d39445a41512e6a706567" data-canonical-src="https://miro.medium.com/proxy/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg" style="max-width:100%;height:auto;">
      <p>Convolutional Neural Networks have 2 main components:</p>
      <ul>
  <li><b>Feature learning:</b> We see convolution, ReLU,Pooling layer phases here. Edges,shades,lines,curves, in this feature learning step get extracted.</li>
  <li><b>Classification:</b> We see Fully Connected(FC) layer in this phase. They will assign a probability for the object on the image according to what our CNN model predicts.</li>
 </ul>
     <p>For more detailed information refer to this <a href="https://medium.com/@purnasaigudikandula/a-beginner-intro-to-convolutional-neural-networks-684c5620c2ce" rel="nofollow">link.</a></p>
      <h2>Our model:</h2>
      <img style="max-width:100%;height:auto;" src="https://raw.githubusercontent.com/Learn-Write-Repeat/cv/main/Face%20Expression%20Recognition/Kshitij/model.png">
      <p>This model has been taken from a
      <a href="http://cs231n.stanford.edu/reports/2016/pdfs/005_Report.pdf" rel="nofollow">research paper</a> by Stanford University on finding the best model for facial expression recognition.</p>
      <b>It consists of 4 convolutional blocks, each comprising of:</b>
      <ul>
  <li><b>Convolutional layer</b> of a number of filters (increaing by powers of 2 in each layer) for feature extraction</li>
  <li><b>Batch Normalisation</b> for normalising the inputs fed into the next layer for avoiding covariate shift.</li>
  <li><b>Maxpooling</b> for extracting the most prominent feature in the feature maps produced after applying the convolution operations and reducing the spatial size of the representation to reduce the amount of parameters and computation in the network</li>
  <li><b>Rectified Linear Unit Activation Function</b> for allowing the model to create complex mappings between the network‚Äôs inputs and outputs, which are essential for learning and modeling complex data, such as images and data sets like these which are non-linear and have high dimensionality.</li>
  <li><b>Dropout</b> for randomly shutting of some neurons in the layer for avoiding overfitting to the data i.e. to prevent our network from learning the deviations in the training data so well, that the network does not generalise on new, unseen data.</li>
  </ul><br>
      <p>The extracted features by the CNN is now passed through <b>two fully connected layers</b> with the same architecture as the convolutional blocks and then a <b>softmax</b> to make the final prediction on the basis of the probabilities of the 7 emotions predicted.</p>
      <h2> Functions, Validation Accuracy:</h2>
      <p><b>The graphs of the loss-function and the accuracy vs the epochs for the training and testing set are shown below:</b></p>
      <img src="https://raw.githubusercontent.com/Learn-Write-Repeat/cv/main/Face%20Expression%20Recognition/Kshitij/Loss_Functions_Accuracy_FER.png" style="max-width:100%;height:auto">
      <h2>The model.py script</h2>
      <p>This python script utilises the saved model and its weights and outputs the prediction out of the given list of emotions. This script creates a model object and defines its corresponding functions like .predict_emotion in a class called FacialExpressionModel, which will be used by the camera.py python script as described below, to output the model's prediction after detecting the face.</p>
      <p><a style="color:blue;" href="https://github.com/Learn-Write-Repeat/cv/blob/main/Face%20Expression%20Recognition/Kshitij/model.py">model.py</a></p>
      <h2>The camera.py script</h2>
      <p>We use OpenCV's cv2 library and the haarcascade_frontalface_default.xml file in this to detect the face, grayscale it(since our model only understands grayscale images), put a rectangular box around it and output the corresponding prediction beside the rectangular box. We will use the model we trained through its json file we saved. We will put the the value in cv2.videocapture as 0, so that we can make prediction through webcam in real time video.</p>
      <p><a style="color:blue;" href="https://github.com/Learn-Write-Repeat/cv/blob/main/Face%20Expression%20Recognition/Kshitij/camera.py">camera.py</a></p>
      <h2>The main.py script</h2>
      <p>After we have detected faces using haarcascade and created a webpage to display video, we will integrate these two modules with our server, using flask and display it in our localhost. This is the script that integrates all other files we have created in this project. This will be executed in the terminal to get the final output.</p>
      <p><a style="color:blue;" href="https://github.com/Learn-Write-Repeat/cv/blob/main/Face%20Expression%20Recognition/Kshitij/main.py">main.py</a></p>
      <p>Implementation in <a style="color:blue;" href="https://github.com/Learn-Write-Repeat/cv/blob/main/Face%20Expression%20Recognition/Kshitij/Facial_Expression_Training.ipynb">Jupyter Notebook</a></p>
      <h1> Some screenshots of the final output in our browser : </h1>
      
      <img style="max-width:100%;height:auto;"src="https://raw.githubusercontent.com/Learn-Write-Repeat/cv/main/Face%20Expression%20Recognition/Kshitij/Angry_Face_Detection.png" ></img>
      <img style="max-width:100%;height:auto;" src="https://raw.githubusercontent.com/Learn-Write-Repeat/cv/main/Face%20Expression%20Recognition/Kshitij/Screenshot%202020-09-20%20at%208.04.15%20PM.png" ></img>
     
      
      
      
      <br>
      <br>
      <p>So you have completed this topic. Congratulations <g-emoji class="g-emoji" alias="trophy" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png">üèÜ</g-emoji><g-emoji class="g-emoji" alias="trophy" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png">üèÜ</g-emoji>.
      <br>Hope you learnt something new
      <br>Happy Learning!! üìö</p>
   </div>


</div></div>
                </div>
            </div>
        </div>
    </section>
    <footer class="d-flex flex-column justify-content-center align-items-center" id="footer" style="padding-bottom: 0;">
        <h1>DevIncept.codes</h1>
        <h5>Contact us:</h5>
        <p>Email:<a href="#">&nbsp;support@devincept.tech</a></p>
        <div class="d-flex flex-row" id="social-button"><button class="btn btn-primary" type="button"><a href="https://www.linkedin.com/company/devincept/" target="_blank"><i class="fa fa-linkedin-square" style="font-size: 24px;" href=""></i></a></button><button class="btn btn-primary" type="button"><a href="https://www.facebook.com/DevIncept/" target="_blank"><i class="fa fa-facebook" style="font-size: 24px;"></i></a></button>
            <button
                class="btn btn-primary" type="button"><a href="https://www.instagram.com/devincept.tech/?hl=en" target="_blank"><i class="fa fa-instagram" style="font-size: 24px;"></i></a></button><button class="btn btn-primary" type="button"><a href="https://github.com/DevIncept" target="_blank"><i class="fa fa-github" style="font-size: 24px;"></i></a></button></div>
    </footer>
<script>
function cb(response) {
    document.getElementById('visits').innerText = response.value;
}
</script>
<script async src="https://api.countapi.xyz/hit/kshitij/visits?callback=cb"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
</body>

</html>
